{"cells":[{"metadata":{"dc":{"key":"3"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"cell_type":"markdown","source":"## # Introduction\n<p><img src=\"https://assets.datacamp.com/production/project_1010/img/book_cover.jpg\" alt=\"The book cover of Peter and Wendy\" style=\"width:183;height:253px;\"></p>\n<h3 id=\"flyawaywithpeterpan\">Fly away with Peter Pan!</h3>\n<p>Peter Pan has been the companion of many children, and went a long way, starting as a Christmas play and ending up as a Disney classic. Did you know that although the play was titled \"Peter Pan, Or The Boy Who Wouldn't Grow Up\", J. M. Barrie's novel was actually titled \"Peter and Wendy\"? </p>\n<p>You're going to explore and analyze Peter Pan's text to answer the question in the instruction pane below. You are working with the text version available here at <a href=\"https://www.gutenberg.org/files/16/16-h/16-h.htm\">Project Gutenberg</a>. Feel free to add as many cells as necessary. Finally, remember that you are only tested on your answer, not on the methods you use to arrive at the answer!</p>\n<p><strong>Note:</strong> If you haven't completed a DataCamp project before you should check out the <a href=\"https://projects.datacamp.com/projects/33\">Intro to Projects</a> first to learn about the interface. <a href=\"https://www.datacamp.com/courses/intermediate-importing-data-in-python\">Intermediate Importing Data in Python</a> and <a href=\"https://www.datacamp.com/courses/introduction-to-natural-language-processing-in-python\">Introduction to Natural Language Processing in Python</a> teach the skills required to complete this project. Should you decide to use them, English stopwords have been downloaded from <code>nltk</code> and are available for you in your environment.</p>"},{"metadata":{"dc":{"key":"3"}},"cell_type":"markdown","source":"## 1. Importing modules"},{"metadata":{"dc":{"key":"3"},"tags":["sample_code"],"collapsed":true,"trusted":true},"cell_type":"code","source":"# Use this cell to begin your analysis, and add as many as you would like!\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n","execution_count":1,"outputs":[]},{"metadata":{"dc":{"key":"3"}},"cell_type":"markdown","source":"## 2. Gathering data\n\nThe data comes from making a request to a specific URL. I used the Python package BeautifulSoup for webs scrapping that can scrap the website content. "},{"metadata":{"dc":{"key":"3"},"collapsed":true,"trusted":true},"cell_type":"code","source":"# Creating a variable for the URL\nURL = \"https://www.gutenberg.org/files/16/16-h/16-h.htm\"\n\n# Requesting the page using requests.get\npage = requests.get(URL)\n\n# Using BeautifulSoup to scrap the page content \nsoup = BeautifulSoup(page.content, 'html.parser')\n\n# Getting only the text using soup.text\ntexts = soup.text","execution_count":13,"outputs":[]},{"metadata":{"dc":{"key":"3"}},"cell_type":"markdown","source":"## 2. Tokenizing the text\nIn this step, we are going to tokenize the text using regex, in which we will tokeinze the words only. Next, we will convert the whole text into lower case in order not to count same words, like \"One\" and \"one\", separately. "},{"metadata":{"dc":{"key":"3"},"trusted":true},"cell_type":"code","source":"# Creating a tokenizer regex variable that tokenizes words only\ntokenizer = nltk.tokenize.RegexpTokenizer(\"\\w+\")\n\n# Tokenizing text\ntxt_tokenized = tokenizer.tokenize(texts)\n\n# Converting words into lowercase\ntext_lower =[word.lower() for word in txt_tokenized]\n\n# printing the Length\nlen(text_lower)","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"51893"},"metadata":{}}]},{"metadata":{"dc":{"key":"3"},"trusted":true},"cell_type":"code","source":"top_ten = Counter(text_lower).most_common(10)\ntop_ten","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"[('the', 2546),\n ('and', 1491),\n ('to', 1285),\n ('he', 1060),\n ('a', 996),\n ('of', 989),\n ('was', 928),\n ('it', 840),\n ('in', 743),\n ('that', 645)]"},"metadata":{}}]},{"metadata":{"dc":{"key":"3"},"collapsed":true},"cell_type":"markdown","source":"## 3. Removing stopwords\nWhile checking the top 10 most common words, I found that they are mainly about english stopwords like \"the\", \"and\", and \"to\" which are not useful for understanding anything about the context. The next step is to remove these words using the stopwords.words('english') method. "},{"metadata":{"dc":{"key":"3"},"trusted":true},"cell_type":"code","source":"# Defining the stop words\nstop_words = set(stopwords.words('english'))\n\n# Filtered words\nfitered_words = [word for word in text_lower if word not in stop_words]\n\nlen(fitered_words)","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"24291"},"metadata":{}}]},{"metadata":{"dc":{"key":"3"},"trusted":true},"cell_type":"code","source":"top_ten = Counter(fitered_words).most_common(10)\ntop_ten","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"[('peter', 410),\n ('wendy', 362),\n ('said', 358),\n ('would', 219),\n ('one', 214),\n ('hook', 175),\n ('could', 142),\n ('cried', 136),\n ('john', 133),\n ('time', 126)]"},"metadata":{}}]},{"metadata":{"dc":{"key":"3"},"trusted":true,"collapsed":true},"cell_type":"code","source":"protagonists = ['peter', 'wendy', 'hook', 'john']\n","execution_count":19,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}